{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello!","text":"<p>Welcome to my personal website. I built this site using Mkdocs, documentation as code, and GitHub resources. My intentions with this page are to document work and trainings that I have successfully completed as well as hold some of my notes. I plan to use this as a resource as well as to benefit my career development.</p>"},{"location":"#about-me","title":"About Me","text":"<p>My name is Kaelyn Melton and I am glad you have decided to browse my page. </p> <p>I live in Arkansas with my husband and our son, who was born in May 2024. Outside of work, I enjoy cooking, running, and spending time outdoors.</p> <p>As for education, I hold the following degrees:</p> <ul> <li>Bachelor's of Science degree in Business Data Analytics</li> <li>Bachelor's of Science in Applied Statistics with Data Science</li> <li>Minor in Mathematics</li> </ul> <p>Currently, I work as an Associate Professional Application Designer at Gainwell Technologies, where I have automated multiple reporting processes using Python, developed and maintained technical documentation, and contributed to project management. My role also involves conducting client training sessions and analyzing process automations to create valuable data insights and visualizations.</p> <p>I have over two years of experience in data and process analysis. I am proficient in Python, Microsoft Office, and Git, and am skilled in technical documentation, data analysis, project management, and data visualization. I have a strong ability to communicate effectively and deliver results, with a focus on maintaining organization and accuracy in my work.</p>"},{"location":"Notes/Technical/api/","title":"APIs","text":"<p>Application Programming Interfaces</p> <p>GUI - Graphical User </p> <p>REST- Representational State Transfer</p> <p>Constraints required for an API to be considered RESTful:  - Client Server Architecture - Statelessness - Layered System - Cacheability - Uniform Design - Code on Demand</p> <p>HTTP - HyperText Transfer Protocol</p>"},{"location":"Notes/Technical/github/","title":"Github","text":""},{"location":"Notes/Technical/github/#cloning-repository-with-sso-authentication-requirements","title":"Cloning Repository with SSO Authentication Requirements","text":"<p>Using HTTPS to clone in Visual Studio does not correctly work when there are saml sso permission requirements on the GitHub repository.</p> <p>An alternative is generating ssh keys. To do so, follow the steps below:</p> <ol> <li>Generate new SSH Key  Type \"ssh-keygen\" in terminal. </li> </ol> <pre><code>ssh-keygen\n</code></pre> <p>Then either pick a location for the key or press enter for default.  Next, enter a passcode. You must do this due to the sso restrictions.</p> <ol> <li>Now open the ssh folder within File Explorer and open the config file.  Add the following information:</li> </ol> <pre><code>Host github.com\nHostName github.com\n User git\n IdentityFile ~/.ssh/id_rsa\n</code></pre> <ol> <li>The next step is to add the key to github.  Type \"cat ~/.ssh/id_rsa.pub\" in the terminal. </li> </ol> <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> <p>Copy the output  Now go to Github.com -&gt; Settings -&gt; SSH and GPG keys.  Click \"New SSH key\".  Then add a title and paste the output into the \"Key\" spot.</p> <ol> <li>Next, copy the SSH information from the desired repository.  Type \"git clone \"enter copied material\"\" into the terminal.</li> </ol> <p>You should then be able to open the repository remotely.</p>"},{"location":"Notes/Technical/kubernetes/","title":"Kubernetes","text":""},{"location":"Notes/Technical/linux/","title":"Linux","text":""},{"location":"Notes/Technical/linux/#ls","title":"ls","text":"<ul> <li>List of content in currently directory</li> <li>colorized options<ul> <li>ls --color=no</li> <li>ls --color=yes</li> <li>ls --color=auto</li> </ul> </li> </ul>"},{"location":"Notes/Technical/linux/#clear","title":"clear","text":"<ul> <li>clears the screen</li> </ul>"},{"location":"Notes/Technical/linux/#-","title":"-","text":"<ul> <li>One dash informs the system that we will pass one letter argument</li> <li>Two dashes means that the argument will contain more than one letter</li> </ul>"},{"location":"Notes/Technical/web_development/","title":"Web Development","text":"<ul> <li>HTML: provides the structure</li> <li>CSS: provides style</li> <li>JavaScript: allows for interactivity</li> </ul>"},{"location":"Notes/Technical/web_development/#html","title":"HTML","text":"<p>HyperText Markup Language</p> <ul> <li>h1: level one header</li> <li>p: paragraph</li> <li>: anchor, hyperlink reference (web address)</li> <li>table: creates a table<ul> <li>thead: table head</li> <li>tr: table row</li> <li>th: header within table</li> <li>tbody: table body</li> <li>td: table data</li> </ul> </li> </ul> <p>HTML Elements Reference{ :target=\"_blank\" rel=\"noopener\" }</p>"},{"location":"Notes/Technical/web_development/#document-object-model","title":"Document Object Model","text":"<p>(DOM)  A document object is what the browser uses to understand the object as a whole.</p>"},{"location":"Notes/Technical/Fixes%20and%20Tricks/html/","title":"HTML","text":""},{"location":"Notes/Technical/Fixes%20and%20Tricks/html/#sharepoint-links","title":"SharePoint Links","text":"<p>Adding a link from a Word document on Sharepoint that goes to a specific page:  - Save link from actual file location on SharePoint - Add &amp;action=embedview&amp;wdStartOn=3 to the end of the link, following mobiledirect=true - Be sure to also delete &amp;action=default from original link</p>"},{"location":"Notes/Technical/Python/table_conversion/","title":"Markdown Table Reformatting and Conversion","text":"<p>This script was used to reformat Markdown files that had been created using CA DBO Reference Guides. All of the material had been formatted within tables, but it was later decided to take the content out of the tables to better fit MkDocs. Since there were hundreds of files to go through, this script completed most of the reformatting work so that only small additional changes were required.</p> <pre><code>  # Reformatting Tables into Headings and Content\n\n  ## Code should iterate through files in selected folder and identify the table. \n  The left column of the table should be converted to second-level headings and the right column of the table should be \n  converted to the contents within the associated heading.\n\n  import re\n\n  # function to find table and the split the table as requested and then rewrite within the file\n  def format_table_in_file(file_path):\n      with open(file_path, 'r', encoding='utf-8') as file:\n          file_contents = file.read()\n\n      # Extract the table portion from the file content\n      table_match = re.search(r'\\| \\*\\*Field\\*\\*[\\s\\S]*?(?=\\n\\n|$)', file_contents)\n      if not table_match:\n          print(\"Table not found in the file.\")\n          return\n\n      table_text = table_match.group(0)\n\n      # Split the table text into lines\n      lines = table_text.strip().split(\"\\n\")\n\n      # Extract the data\n      data_lines = lines[2:]  # Skip the header and the line separating header and data\n      data = [line.split('|') for line in data_lines]\n\n      # Format the table into desired format\n      formatted_table = \"\"\n      for row in data:\n          if len(row) &gt;= 2:\n              field = row[1].strip()  # Remove leading and trailing whitespace\n              description = row[2].strip() if len(row) &gt;= 3 else \"\"  # Handle cases where description might be missing\n              formatted_table += f\"## {field}\\n\\n{description}\\n\\n\"\n\n      # Replace the original table in the file with the formatted table\n      modified_contents = re.sub(re.escape(table_text), formatted_table, file_contents)\n\n      # Write the modified contents back to the file\n      with open(file_path, 'w', encoding='utf-8') as file:\n          file.write(modified_contents)\n\n      print(\"Table successfully formatted and written to the file.\")\n\n  import os\n\n  def format_tables_in_folder(folder_path):\n      # Iterate over all files in the folder\n      for filename in os.listdir(folder_path):\n          # Check if the file is a Markdown file\n          if filename.endswith(\".md\"):\n              file_path = os.path.join(folder_path, filename)\n              format_table_in_file(file_path)\n\n  # Specify the folder path containing the Markdown files\n  folder_path = \"docs/Suspense and Error Manual/Recipient Edits\"\n\n  # Format tables in all files in the folder\n  format_tables_in_folder(folder_path)\n</code></pre> <p>Example of table before the conversion</p> <p></p> <p>Example of table after the conversion</p> <p></p> <p>3/26/2024 </p>"},{"location":"Projects/alaska_reporting/","title":"Process Improvement Reporting Project","text":"<p>This project entails bringing in three different datasets and producing a result file that contained matches from the various datasets based on specific columns.</p> <p>the original datasets did not have consistent naming and required cleanup in order to perform the matching process.</p> <p>the resulting document had to be produced in a specific format</p> <p>Prior to producing this script, the matching was done manually by individually checking each row from the datasets. This job was tedious as there were thousands of rows with more being continuously added. It required a large amount of time that could be spent doing a more valuable task. This process improvement saved hours of time each week and improved efficiency in achieving results.</p> <p>Also please note that the names of the datasets have been changed and \"***\" or alphabet letters are serving as replacements in order to maintain confidentiality.</p>"},{"location":"Projects/alaska_reporting/#python-script","title":"Python Script","text":"<p>''' the objective of this script is to produce a file containing data matches from multiple datasets for the benefit of a specific team.</p> <p>This file first reads in the following datasets:</p> <ul> <li>\"***.csv\" </li> <li>\"***.csv\"</li> <li>\"***.csv\"</li> </ul> <p>And tan proceeds with cleaning and normalizing the data for comparisons.</p> <p>the *** and *** reports are being compared to the *** data based on the following information: </p> <ul> <li>first and last name</li> <li>business name</li> <li>address</li> </ul> <p>If any of tase values are found to be a match between *** or *** and the *** data, the row is added to a report based on an inner merge and includes the following columns:</p> <ul> <li>\"***_first_name\"</li> <li>\"***_last_name\"</li> <li>\"***_business_name\"</li> <li>\"***_ID\"</li> <li>\"***_ID\"</li> <li>\"***_ALT_ID\"</li> <li>\"comes_from\"</li> <li>\"matcad_on\"</li> </ul> <p>the final report along with the three originial datasets are tan exported to an xlsx file called: \"resulting_report-b_c_export.xlsx\".  the date may be included in the title of the resulting file as well.</p> <p>import pandas as pd import re</p>"},{"location":"Projects/alaska_reporting/#functions","title":"Functions","text":""},{"location":"Projects/alaska_reporting/#read_in_files-reads-in-each-of-the-necessary-files-for-comparison","title":"\"read_in_files\" reads in each of the necessary files for comparison","text":"<pre><code>def read_in_files(a, b, c):\n\n# A initial report\na = pd.read_csv(a, index_col = False)\n# B initial report\nb = pd.read_csv(b, index_col = False)\n# C initial report\nc = pd.read_csv(c, index_col = False)\n\nreturn a, b, c\n</code></pre>"},{"location":"Projects/alaska_reporting/#remove_special_characters-removes-special-characters-which-aids-in-normalizing-certain-columns-within-the-data","title":"\"remove_special_characters\" removes special characters which aids in normalizing certain columns within the data","text":"<pre><code>def remove_special_characters(value):\n\n# reg expression pattern\npattern = r'[^a-zA-Z0-9\\s]'\n\n# remove special characters and return df\nreturn re.sub(pattern, '', value)\n</code></pre>"},{"location":"Projects/alaska_reporting/#normalize_address-focuses-on-the-address-columns-and-substitutes-common-abbreviations-to-better-normalize-address-values","title":"\"normalize_address\" focuses on the address columns and substitutes common abbreviations to better normalize address values","text":"<pre><code>def normalize_address(address):\n\n# convert abbreviations to full forms\naddress = re.sub(r'\\bST\\b', 'Street', address, flags=re.IGNORECASE)\naddress = re.sub(r'\\bAVE\\b', 'Avenue', address, flags=re.IGNORECASE)\naddress = re.sub(r'\\bDR\\b', 'Drive', address, flags=re.IGNORECASE)\naddress = re.sub(r'\\bPWY\\b', 'Parkway', address, flags=re.IGNORECASE)\naddress = re.sub(r'\\bSTE\\b', 'Suite', address, flags=re.IGNORECASE)\naddress = re.sub(r'\\bAPT\\b', 'Apartment', address, flags=re.IGNORECASE)\n\nreturn address\n</code></pre>"},{"location":"Projects/alaska_reporting/#extract_alphabetical-aids-in-matching-address-columns-by-searching-each-value-for-the-first-string-of-alphabetical-characters","title":"\"extract_alphabetical\" aids in matching address columns by searching each value for the first string of alphabetical characters","text":"<pre><code>def extract_alphabetical(string):\n\nmatch = re.search(r'[a-zA-Z]+', string)\nif match:\n    return match.group().strip()\nelse:\n    return None\n</code></pre>"},{"location":"Projects/alaska_reporting/#sort-sorts-the-resulting-data-to-better-format-the-output","title":"\"sort\" sorts the resulting data to better format the output","text":"<pre><code>def sort(group):\n\nreturn group.sort_values(by='***_last_name')\n</code></pre>"},{"location":"Projects/alaska_reporting/#report-executes-each-of-the-above-functions-ware-necessary-as-well-as-performs-the-remaining-actions-to-produce-the-final-report","title":"\"report\" executes each of the above functions ware necessary as well as performs the remaining actions to produce the final report","text":"<pre><code>def report(a, b, c, file_path):\n\n# reading in files\na, b, c = read_in_files(a, b, c)\n# creating copies of each file so that the originals are not altered.\na_original = a.copy()\nb_original = b.copy()\nc_original = c.copy()\n\n# normalizing *** first name and changing column name to \"***_first_name\"\na[\"***_first_name\"] = a[\"*_FIRST_NAM\"].str.upper()\nb[\"***_first_name\"] = b[\"FIRSTNAME\"].str.upper()\nc[\"***_first_name\"] = c[\"First\"].str.upper()\n\n# normalizing *** last name and changing column name to \"***_last_name\" \na[\"***_last_name\"] = a[\"*_LAST_NAM\"].str.upper()\nb[\"***_last_name\"] = b[\"LASTNAME\"].str.upper()\nc[\"***_last_name\"] = c[\"Last\"].str.upper()\n\n# normalizing *** business name and changing column name to \"***_business_name\"\na[\"***_business_name\"] = a[\"*_DBA_NAM\"].str.upper()\nb[\"***_business_name\"] = b[\"BUSNAME\"].str.upper()    \nc[\"***_business_name\"] = c[\"Name\"].str.upper()\n\n# normalizing addresses and changing column name to \"address\"\n# also concatenating some strings and replacing some nans in tase columns to improve format of addresses for comparison\na[\"address\"] =  a[\"P_FULL_ADR\"].astype(str).apply(remove_special_characters).apply(normalize_address)\na['address'] = a['address'].str.replace(r' (\\d{4})$', r'', regex=True)\nb[\"ZIP\"] = b[\"ZIP\"].astype(str)\nb[\"address\"] = b[\"ADDRESS\"] + \" \" + b[\"CITY\"] + \" \" + b[\"STATE\"] + \" \" + b[\"ZIP\"] \nb[\"address\"] =  b[\"address\"].astype(str).apply(remove_special_characters).apply(normalize_address)\nc[\"Address 3\"] = c[\"Address 3\"].astype(str)\nc[\"Address 4\"] = c[\"Address 4\"].astype(str)\nc.replace({'nan': '', 'XX': ''}, inplace=True)\nc[\"address\"] = c[\"Address 1\"].fillna(\"\") + \" \" + c[\"Address 2\"].fillna(\"\") + \" \" + c[\"Address 3\"].fillna(\"\") + \" \" + c[\"Address 4\"].fillna(\"\") + \" \" + c[\"City\"].fillna(\"\") + \" \" + c[\"State / Province\"].fillna(\"\") + \" \" + c[\"Zip Code\"].fillna(\"\")\nc[\"address\"] =  c[\"address\"].astype(str).apply(remove_special_characters).apply(normalize_address)\n\n# producing zip code column and naming \"zip_code\"\na[\"zip_code\"] = a[\"address\"].str.extract(r'(\\d+)$')\nb[\"zip_code\"] = b[\"address\"].str.extract(r'(\\d+)$')\nc[\"zip_code\"] = c[\"address\"].str.extract(r'(\\d+)$')\n\n# producing address number column and naming \"address_number\"\n# This column is the first number to appear in the address column aiding in matching addresses.\na['address_number'] = a['address'].str.extract(r'^(\\d+)')\nb['address_number'] = b['address'].str.extract(r'^(\\d+)')\nc['address_number'] = c['address'].str.extract(r'^(\\d+)')\n\n# producing address string column and naming \"address_string\"\n# This column is the first string of characters to appear in the address column aiding in matching addresses.\na['address_string'] = a['address'].apply(lambda x: extract_alphabetical(x))\nb['address_string'] = b['address'].apply(lambda x: extract_alphabetical(x))\nc['address_string'] = c['address'].apply(lambda x: extract_alphabetical(x))\n\n# changing data types to better fit values\na[\"***_ID\"] = a[\"***_ID\"].astype(str)\na[\"***_ID\"] = a[\"***_ID\"].astype(str)\na[\"***_ALT_ID\"] = a[\"***_ALT_ID\"].astype(str)\n\n# Merges\n# Using the new and cleaned up columns, merges are made based on tase columns between b and a, and c and a.\n# Columns called \"comes_from\" and \"matched_on\" are also created to identify which file the match is from (b or c) and\n# which columns the match is from (*** name, *** business name, address)\n\n# merging based on *** first and last name\ncolumns_to_keep = ['***_first_name', \"***_last_name\"]\nb_***_name = b[columns_to_keep].drop_duplicates().dropna(subset=[\"***_first_name\", \"***_last_name\"])\nc_***_name = c[columns_to_keep].drop_duplicates().dropna(subset=[\"***_first_name\", \"***_last_name\"])\n# merging b with a on inner merge\nb_***_name = pd.merge(a, b_***_name, on=[\"***_first_name\", \"***_last_name\"], how='inner').drop_duplicates()\nb_***_name[\"comes_from\"] = \"b\"\nb_***_name[\"matcad_on\"] = \"*** name\"\n# merging c with a on inner merge\nc_***_name = pd.merge(a, c_***_name, on=[\"***_first_name\", \"***_last_name\"], how='inner').drop_duplicates()\nc_***_name[\"comes_from\"] = \"c\"\nc_***_name[\"matcad_on\"] = \"*** name\"\n\n# merging based on *** business name\ncolumns_to_keep = [\"***_business_name\"]\nb_***_business_name = b[columns_to_keep].drop_duplicates().dropna(subset=[\"***_business_name\"])\nc_***_business_name = c[columns_to_keep].drop_duplicates().dropna(subset=[\"***_business_name\"])\n# merging b with a on inner merge\nb_***_business_name = pd.merge(a, b_***_business_name, on=[\"***_business_name\"], how='inner').drop_duplicates()\nb_***_business_name[\"comes_from\"] = \"b\"\nb_***_business_name[\"matcad_on\"] = \"*** business name\"\n# merging c with a on inner merge\nc_***_business_name = pd.merge(a, c_***_business_name, on=[\"***_business_name\"], how='inner').drop_duplicates()\nc_***_business_name[\"comes_from\"] = \"c\"\nc_***_business_name[\"matcad_on\"] = \"*** business name\"\n\n# merging based on address\n# keeping only the columns from b and c that relate to obtaining possible address matches\ncolumns_to_keep = [\"zip_code\", \"address_number\", \"address\", \"address_string\"]\nb_address = b[columns_to_keep].drop_duplicates().dropna(subset=[\"zip_code\"])\nc_address = c[columns_to_keep].drop_duplicates().dropna(subset=[\"zip_code\"])\n# merging b with a on inner merge\nb_address = pd.merge(a, b_address, on=[\"zip_code\", \"address_number\", \"address_string\"], how='inner').drop_duplicates()\nb_address[\"comes_from\"] = \"b\"\nb_address[\"matcad_on\"] = \"address\"\n# merging c with a on inner merge\nc_address = pd.merge(a, c_address, on=[\"zip_code\", \"address_number\", \"address_string\"], how='inner').drop_duplicates()\nc_address[\"comes_from\"] = \"c\"\nc_address[\"matcad_on\"] = \"address\"\n\n# concatenating the merged dataframes so that all matches are contained within one set of data\nresult = pd.concat([c_***_name, b_***_name, c_***_business_name, b_***_business_name, b_address, c_address])\n\n# keeping only columns necessary for output file\ncolumns_to_keep = ['***_first_name', \"***_last_name\", \"***_ID\", \"***_business_name\", \"***_ID\", \"***_ALT_ID\", \"comes_from\", \"matched_on\"]\nresult = result[columns_to_keep].drop_duplicates()\n\n# grouping the data by *** alt ID and tan sorting the data by last name\ngrouped = result.groupby(\"***_ALT_ID\")\n# applying the sorting function to each group and tan concatenate and substiture NaN with blank value\nsorted_result = pd.concat([sort(group) for _, group in grouped]).fillna(\"\")\n\n# exporting output file to listed file path\nwith pd.ExcelWriter(file_path, engine=\"xlsxwriter\",) as writer:\n    c_original.to_excel(writer, sheet_name = \"c\", index = False)\n    b_original.to_excel(writer, sheet_name = \"b\", index = False)\n    a_original.to_excel(writer, sheet_name = \"a\", index = False)\n    sorted_result.to_excel(writer, sheet_name = \"Matches\", index = False)\n\nprint(\"File Exported\")\n</code></pre> <p>the following variables can be altered based on the names and locations of the files being imported as well as the expected title and location of the resulting report. </p> <p>the \"file_path\" variable associates with ware the resulting report will be located and what the file will be titled as.</p> <p>the \"a\" variable should include the location and file name for the a extract. the following columns are expected:</p> <ul> <li>***_NAM</li> <li>***_DBA_NAM</li> <li>***_FIRST_NAM</li> <li>***_LAST_NAM</li> <li>***_ALT_ID </li> <li>***_ID</li> <li>***_ID</li> <li>***_ID</li> <li>***_FULL_ADR</li> </ul> <p>the \"b\" variable should include the location and file name for the extract from b. the following columns are expected:</p> <ul> <li>LASTNAME</li> <li>FIRSTNAME</li> <li>MIDNAME</li> <li>BUSNAME</li> <li>GENERAL</li> <li></li> <li>UPIN</li> <li></li> <li>DOB</li> <li>ADDRESS</li> <li>CITY</li> <li>STATE</li> <li>ZIP</li> <li>EXCLTYPE</li> <li>EXCLDATE</li> <li>***DATE</li> <li>***DATE</li> <li>***STATE</li> </ul> <p>the \"c\" variable should include the location and file name for the extract from c. the following columns are expected:</p> <ul> <li></li> <li>Name</li> <li>Prefix</li> <li>First</li> <li>Middle</li> <li>Last</li> <li>Suffix</li> <li>Address 1</li> <li>Address 2</li> <li>Address 3</li> <li>Address 4</li> <li>City</li> <li>State / Province</li> <li>Country</li> <li>Zip Code</li> <li></li> <li>Blank (Deprecated)</li> <li>Unique Entity ID</li> <li>Exclusion Program</li> <li>CT Code</li> <li>Exclusion Type</li> <li>Additional Comments</li> <li>*** Date</li> <li>*** Date</li> <li>Record Status</li> <li>Cross-Reference</li> <li>*** Number</li> <li>CAGE</li> <li></li> <li>Creation_Date</li> </ul> <p>\"\"\"</p> <p>a = \"*.csv\"</p> <p>b = \"*.csv\"</p> <p>c = \"*.csv\"</p> <p>file_path = '***.xlsx'</p>"},{"location":"Projects/alaska_reporting/#calling-functions","title":"calling functions","text":"<p>report(a, b, c, file_path) '''</p>"},{"location":"Projects/automation_value_analysis/","title":"Value Analysis of Automation","text":"<p>This project involved a value analysis of a digital automation process, focusing on quantifying the manual work completed by a bot relative to the total work. Utilizing Power BI, Python, and Excel, the analysis covered a process with 8 automated steps. The bot's performance varied based on task complexity, with the potential to complete up to 100% of the work. The analysis evaluated the extent of automation versus manual intervention and the results allow for time studies of manual work to calculate metrics such as FTEs saved and cost savings. Provided below are highlights and keys steps that I took to deliver actionable results.</p>"},{"location":"Projects/automation_value_analysis/#initial-data","title":"Initial Data","text":"<p>There were multiple sources and files used to build the analysis for this project. Data from the team manually working the task and data produced from the digital bot were required to accurately produce metrics.</p> <p>Collecting the right data as well as cleaning the data were the first steps in the process. Excel allowed for ease in viewing the data in early stages to better understand which columns were of importance and how I wanted to approach my analysis. </p> <p>I collaborated with both technical and business teams to gain a well-rounded understanding of the automated process and data that I was working with. This cross-functional approach allowed me to set my analysis up for success. Viewing the data in Excel first provided valuable insights that guided me in asking the right questions and having productive discussions . </p> <p>The screenshots below provide examples of the data I worked with. While the data was already in solid shape, some cleaning and normalizing still had to take place. </p> <p></p> <p></p>"},{"location":"Projects/automation_value_analysis/#manipulating-data-and-performing-calculations","title":"Manipulating Data and Performing Calculations","text":"<p>Once the data was collected and validated, I applied data cleaning and transformation skills using Python to further refine the datasets and develop functions that helped drive an insightful analysis. This process allowed me to prepare the data for creating meaningful visualizations.</p> <p>One key function I developed, which is provided below, calculated the percentage of work completed by the bot. With task complexities varying across the eight steps of the process, the bot is not always able to complete one-hundred percent of the process without human intervention. By adding this new column, I was able to produce a clear metric relating to performance which enabled me to generate visualizations that effectively communicated the bot's efficiency.</p> <p></p>"},{"location":"Projects/automation_value_analysis/#visualizing-data","title":"Visualizing Data","text":"<p>Once I had cleaned and manipulated the data as desired, I moved on to Power Bi for building data visualizations. At this point, I worked to answer the questions being asked and tell a story about the data through specific visualizations. </p> <p>One specific question asked was, \"How much of the work completed is performed by the bot?\" The screenshot below shows one way I was able to visualize the answer to this question.</p> <p></p> <p>To best provide visuals that are easy to understand and answer questions as directly as possible, I also created measures with formulas like the one shown below to disply the data as I wanted to.</p> <p></p>"},{"location":"Projects/automation_value_analysis/#further-analysis","title":"Further Analysis","text":"<p>With the questions relating to the automated process performance answered, metrics for cost savings and FTEs saved could now be answered. This provides value by understanding the amount of money saved and diving into next steps to improve the process.</p> <p>The analysis also provided actionable insights that helped the team identify potential bottlenecks in the process and areas where the bot's efficiency could be improved. This paved the way for potential of ongoing monitoring through a live dashboard to ensure continuous optimization.</p>"}]}